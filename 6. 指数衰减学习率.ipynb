{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w_{t+1} = w_{t} - \\text{learning rate}\\nabla$$\n",
    "\n",
    " - $w_{t+1}$:更新后的参数\n",
    " - $w_t$:当前的参数\n",
    " - $\\text{learning_rate}$:学习率\n",
    " - $\\nabla$:损失函数的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eg:损失函数为$loss = (w + 1)^2$, $\\nabla = 2(w + 1)$, 初始的$w$设置为5， 学习率设置为0.2，根据参数更新公式有：\n",
    "\n",
    " - $2.6 = 5 - 0.2 * (2 * 5 + 2)$\n",
    " - $1.16 = 2.6 - 0.2 * (2 * 2.6 + 2)$\n",
    " - $\\cdots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - right learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6 1.16 0.296 -0.22240001 -0.53344 -0.720064 -0.8320384 -0.899223 -0.9395338 -0.9637203 -0.9782322 -0.9869393 -0.9921636 -0.99529815 -0.9971789 -0.99830735 -0.9989844 -0.99939066 -0.9996344 -0.99978065 -0.9998684 -0.999921 -0.9999526 -0.99997157 -0.99998295 -0.99998975 -0.99999386 -0.9999963 -0.9999978 -0.9999987 -0.9999992 -0.9999995 -0.9999997 -0.9999998 -0.9999999 -0.99999994 -0.99999994 -0.99999994 -0.99999994 -0.99999994 "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w = tf.Variable(tf.constant(5, tf.float32))\n",
    "loss = tf.square(w + 1)\n",
    "train = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(40):\n",
    "        sess.run(train)\n",
    "        print(sess.run(w), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - big learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 -7.0 5.0 "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w = tf.Variable(tf.constant(5, tf.float32))\n",
    "loss = tf.square(w + 1)\n",
    "train = tf.train.GradientDescentOptimizer(1).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(40):\n",
    "        sess.run(train)\n",
    "        print(sess.run(w), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - small learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9988 4.9976 4.9964004 4.995201 4.994002 4.992803 4.9916043 4.990406 4.9892077 4.98801 4.986812 4.985615 4.9844174 4.9832206 4.9820237 4.9808273 4.979631 4.978435 4.977239 4.9760437 4.9748483 4.9736533 4.9724584 4.971264 4.9700694 4.9688754 4.9676814 4.966488 4.9652944 4.9641013 4.9629083 4.9617157 4.960523 4.959331 4.958139 4.9569473 4.9557557 4.9545646 4.9533734 4.952183 "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w = tf.Variable(tf.constant(5, tf.float32))\n",
    "loss = tf.square(w + 1)\n",
    "train = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(40):\n",
    "        sess.run(train)\n",
    "        print(sess.run(w), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**指数衰减的学习率**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{learning_rate} = \\text{LEARNING_RATE_BASE} * \\text{LEARNING_RATE_DECAY}^{\\frac{\\text{global_step}}{\\text{LEARNING_RATE_STEP}}}$$\n",
    "\n",
    " - $\\text{LEARNING_RATE_BASE}$:学习率基数\n",
    " - $\\text{LEARNING_RATE_DECAY}$:学习率衰减\n",
    " - $\\text{global_step}$:运行了几轮Batch_size\n",
    " - $\\text{LEARNING_RATE_STEP}$:喂入多少轮batch_size之后更新一次学习率=总样本数/Batch_Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step= 1 learning_rate= 0.1 w= 4.2 loss_val= 10.239999\n",
      "global_step= 2 learning_rate= 0.1 w= 3.56 loss_val= 6.5536\n",
      "global_step= 3 learning_rate= 0.1 w= 3.0479999 loss_val= 4.1943035\n",
      "global_step= 4 learning_rate= 0.1 w= 2.6383998 loss_val= 2.684354\n",
      "global_step= 5 learning_rate= 0.1 w= 2.31072 loss_val= 1.7179868\n",
      "global_step= 6 learning_rate= 0.1 w= 2.0485759 loss_val= 1.0995114\n",
      "global_step= 7 learning_rate= 0.1 w= 1.8388608 loss_val= 0.70368737\n",
      "global_step= 8 learning_rate= 0.1 w= 1.6710886 loss_val= 0.45035988\n",
      "global_step= 9 learning_rate= 0.1 w= 1.5368708 loss_val= 0.2882303\n",
      "global_step= 10 learning_rate= 0.099 w= 1.4294966 loss_val= 0.18446738\n",
      "global_step= 11 learning_rate= 0.099 w= 1.3444563 loss_val= 0.11865015\n",
      "global_step= 12 learning_rate= 0.099 w= 1.2762539 loss_val= 0.07631624\n",
      "global_step= 13 learning_rate= 0.099 w= 1.2215557 loss_val= 0.049086932\n",
      "global_step= 14 learning_rate= 0.099 w= 1.1776876 loss_val= 0.0315729\n",
      "global_step= 15 learning_rate= 0.099 w= 1.1425055 loss_val= 0.020307826\n",
      "global_step= 16 learning_rate= 0.099 w= 1.1142894 loss_val= 0.013062067\n",
      "global_step= 17 learning_rate= 0.099 w= 1.0916601 loss_val= 0.008401582\n",
      "global_step= 18 learning_rate= 0.099 w= 1.0735115 loss_val= 0.005403938\n",
      "global_step= 19 learning_rate= 0.099 w= 1.0589563 loss_val= 0.0034758411\n",
      "global_step= 20 learning_rate= 0.09801 w= 1.0472829 loss_val= 0.002235676\n",
      "global_step= 21 learning_rate= 0.09801 w= 1.0380145 loss_val= 0.0014451046\n",
      "global_step= 22 learning_rate= 0.09801 w= 1.0305629 loss_val= 0.0009340895\n",
      "global_step= 23 learning_rate= 0.09801 w= 1.0245719 loss_val= 0.00060377805\n",
      "global_step= 24 learning_rate= 0.09801 w= 1.0197554 loss_val= 0.0003902744\n",
      "global_step= 25 learning_rate= 0.09801 w= 1.015883 loss_val= 0.0002522687\n",
      "global_step= 26 learning_rate= 0.09801 w= 1.0127696 loss_val= 0.00016306217\n",
      "global_step= 27 learning_rate= 0.09801 w= 1.0102665 loss_val= 0.000105401894\n",
      "global_step= 28 learning_rate= 0.09801 w= 1.008254 loss_val= 6.812936e-05\n",
      "global_step= 29 learning_rate= 0.09801 w= 1.0066361 loss_val= 4.4038392e-05\n",
      "global_step= 30 learning_rate= 0.0970299 w= 1.0053353 loss_val= 2.8465756e-05\n",
      "global_step= 31 learning_rate= 0.0970299 w= 1.0043 loss_val= 1.8489985e-05\n",
      "global_step= 32 learning_rate= 0.0970299 w= 1.0034655 loss_val= 1.2009921e-05\n",
      "global_step= 33 learning_rate= 0.0970299 w= 1.002793 loss_val= 7.800594e-06\n",
      "global_step= 34 learning_rate= 0.0970299 w= 1.0022509 loss_val= 5.066595e-06\n",
      "global_step= 35 learning_rate= 0.0970299 w= 1.0018141 loss_val= 3.2910566e-06\n",
      "global_step= 36 learning_rate= 0.0970299 w= 1.0014621 loss_val= 2.137742e-06\n",
      "global_step= 37 learning_rate= 0.0970299 w= 1.0011784 loss_val= 1.3885884e-06\n",
      "global_step= 38 learning_rate= 0.0970299 w= 1.0009497 loss_val= 9.0200683e-07\n",
      "global_step= 39 learning_rate= 0.0970299 w= 1.0007654 loss_val= 5.8590274e-07\n",
      "global_step= 40 learning_rate= 0.096059605 w= 1.0006169 loss_val= 3.8057556e-07\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE_BASE = 0.1\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "LEARNING_RATE_STEP = 10\n",
    "\n",
    "global_step = tf.Variable(0, trainable = False)\n",
    "learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, LEARNING_RATE_STEP, LEARNING_RATE_DECAY, staircase=True)\n",
    "w = tf.Variable(tf.constant(5, tf.float32))\n",
    "loss = tf.square(w - 1)\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(40):\n",
    "        sess.run(train)\n",
    "        global_step_val = sess.run(global_step)\n",
    "        learning_rate_val = sess.run(learning_rate)\n",
    "        w_val = sess.run(w)\n",
    "        loss_val = sess.run(loss)\n",
    "        print(\"global_step=\", global_step_val, \"learning_rate=\", learning_rate_val, \"w=\", w_val, \"loss_val=\", loss_val)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
